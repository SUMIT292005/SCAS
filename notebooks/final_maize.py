# -*- coding: utf-8 -*-
"""Final_Maize.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuixAfS0dbe3EHDadCqI2fOqozr4rRu-
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# 1. Unzip the dataset
zip_path = "/content/drive/MyDrive/Dataset/archive (15).zip"
extract_path = "/content/Maize_Dataset"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("âœ… Dataset extracted!")

# 2. Browse the folder structure
for root, dirs, files in os.walk(extract_path):
    level = root.replace(extract_path, '').count(os.sep)
    indent = ' ' * 4 * level
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 4 * (level + 1)
    for f in files[:5]:  # show only first 5 files per folder
        print(f"{subindent}{f}")

# 3. Count images per class
from collections import defaultdict

image_counts = defaultdict(int)

for root, dirs, files in os.walk(extract_path):
    class_name = os.path.basename(root)
    image_counts[class_name] += len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])

print("\nðŸ“Š Images per class:")
for cls, count in image_counts.items():
    print(f"{cls}: {count}")

import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img

# Paths
dataset_dir = "/content/Maize_Dataset/data"
output_dir = "/content/Maize_Dataset/balanced_data"

# Make new balanced directory
os.makedirs(output_dir, exist_ok=True)

# Augmentation setup
datagen = ImageDataGenerator(
    rotation_range=25,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    fill_mode="nearest"
)

# Get class counts
class_counts = {cls: len(os.listdir(os.path.join(dataset_dir, cls)))
                for cls in os.listdir(dataset_dir)}

max_count = max(class_counts.values())
print("ðŸ“Š Original class counts:", class_counts, "Target =", max_count)

# Function to balance classes
for cls, count in class_counts.items():
    src_dir = os.path.join(dataset_dir, cls)
    dst_dir = os.path.join(output_dir, cls)
    os.makedirs(dst_dir, exist_ok=True)

    # Copy original images
    for fname in os.listdir(src_dir):
        os.system(f"cp '{os.path.join(src_dir, fname)}' '{os.path.join(dst_dir, fname)}'")

    # If class has fewer images, augment
    if count < max_count:
        print(f"âš¡ Augmenting {cls}: {count} â†’ {max_count}")
        imgs = os.listdir(src_dir)
        i = 0
        while count < max_count:
            img_path = os.path.join(src_dir, imgs[i % len(imgs)])
            img = load_img(img_path, target_size=(224, 224))
            x = img_to_array(img)
            x = np.expand_dims(x, axis=0)

            # Generate 1 augmented image
            for batch in datagen.flow(x, batch_size=1,
                                      save_to_dir=dst_dir,
                                      save_prefix=cls,
                                      save_format='jpg'):
                count += 1
                break
            i += 1

print("\nâœ… Balanced dataset created at:", output_dir)

extract_path = "/content/Maize_Dataset/balanced_data"

# 2. Browse the folder structure
for root, dirs, files in os.walk(extract_path):
    level = root.replace(extract_path, '').count(os.sep)
    indent = ' ' * 4 * level
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 4 * (level + 1)
    for f in files[:5]:  # show only first 5 files per folder
        print(f"{subindent}{f}")

# 3. Count images per class
from collections import defaultdict

image_counts = defaultdict(int)

for root, dirs, files in os.walk(extract_path):
    class_name = os.path.basename(root)
    image_counts[class_name] += len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])

print("\nðŸ“Š Images per class:")
for cls, count in image_counts.items():
    print(f"{cls}: {count}")

import shutil

# Path of balanced dataset
folder_path = "/content/Maize_Dataset/balanced_data"

# Output zip file name
zip_path = "/content/maize_balanced_data.zip"

# Create zip
shutil.make_archive(zip_path.replace(".zip", ""), 'zip', folder_path)

print("âœ… Zipping done:", zip_path)

# Move final dataset ZIP into Google Drive
shutil.move("/content/maize_balanced_data.zip",
            "/content/drive/MyDrive/Dataset/maize_balanced_data.zip")

print("âœ… File saved to Google Drive: /MyDrive/maize_balanced_data.zip")



import tensorflow as tf
import matplotlib.pyplot as plt
import os

# ==============================
# 1. Paths & Parameters
# ==============================
data_dir = "/content/Maize_Dataset/balanced_data"
img_size = (224, 224)  # EfficientNet input size
batch_size = 32
epochs = 15

# ==============================
# 2. Load Dataset
# ==============================
train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=42,
    image_size=img_size,
    batch_size=batch_size
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=42,
    image_size=img_size,
    batch_size=batch_size
)

class_names = train_ds.class_names
print("âœ… Classes:", class_names)

# ==============================
# 3. Performance Optimizations
# ==============================
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# ==============================
# 4. Data Augmentation
# ==============================
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
    tf.keras.layers.RandomContrast(0.1),
])

# ==============================
# 5. Transfer Learning Model
# ==============================
base_model = tf.keras.applications.EfficientNetB0(
    input_shape=img_size + (3,),
    include_top=False,
    weights="imagenet"
)
base_model.trainable = False  # Freeze base layers initially

inputs = tf.keras.Input(shape=img_size + (3,))
x = data_augmentation(inputs)
x = tf.keras.applications.efficientnet.preprocess_input(x)
x = base_model(x, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.3)(x)
outputs = tf.keras.layers.Dense(len(class_names), activation="softmax")(x)

model = tf.keras.Model(inputs, outputs)

# ==============================
# 6. Compile
# ==============================
model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# ==============================
# 7. Train
# ==============================
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

# ==============================
# 8. Fine-Tune (Unfreeze some layers)
# ==============================
base_model.trainable = True
fine_tune_at = len(base_model.layers) - 20  # unfreeze last 20 layers

for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # lower LR
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

history_fine = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5
)

# ==============================
# 9. Evaluate
# ==============================
loss, acc = model.evaluate(val_ds)
print(f"\nâœ… Final Model Accuracy: {acc*100:.2f}%")

# ==============================
# 10. Plot Training Curves
# ==============================
plt.figure(figsize=(8,6))
plt.plot(history.history['accuracy'] + history_fine.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'] + history_fine.history['val_accuracy'], label='Val Acc')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# ==============================
# 11. Save Model
# ==============================
model.save("maize_disease_model.h5")
print("âœ… Model saved as maize_disease_model.h5")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Get true labels & predictions
y_true = []
y_pred = []

for images, labels in val_ds:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

# Report
print("ðŸ“Š Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from tensorflow.keras.preprocessing import image

def predict_image(img_path):
    img = image.load_img(img_path, target_size=img_size)  # Resize to (224,224)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)

    preds = model.predict(img_array)
    predicted_class = class_names[np.argmax(preds)]
    confidence = np.max(preds) * 100

    print(f"ðŸ–¼ï¸ Image: {os.path.basename(img_path)}")
    print(f"âœ… Predicted: {predicted_class} ({confidence:.2f}%)")

    plt.imshow(img)
    plt.axis("off")
    plt.title(f"{predicted_class} ({confidence:.2f}%)")
    plt.show()

# Example usage
predict_image("/content/Maize_Dataset/balanced_data/Gray_Leaf_Spot/Corn_Gray_Spot (104).JPG")
predict_image("/content/Maize_Dataset/balanced_data/Healthy/Corn_Health (1004).jpg")
predict_image("/content/Maize_Dataset/balanced_data/Blight/Blight_0_1553.jpg")
predict_image("/content/Maize_Dataset/balanced_data/Common_Rust/Corn_Common_Rust (1004).JPG")
predict_image("/content/Maize_Dataset/balanced_data/Gray_Leaf_Spot/Corn_Gray_Spot (124).JPG")

# Build inference model (no augmentation)
inference_inputs = tf.keras.Input(shape=(224,224,3))
x = tf.keras.applications.efficientnet.preprocess_input(inference_inputs)
x = base_model(x, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.3)(x)
outputs = tf.keras.layers.Dense(len(class_names), activation="softmax")(x)

inference_model = tf.keras.Model(inference_inputs, outputs)

# Copy weights from trained model
inference_model.set_weights(model.get_weights())

# Save for deployment
inference_model.save("/content/updated_maize_disease_efficientnetb0.h5")
print("âœ… Inference model saved (without augmentation).")



from tensorflow.keras.models import load_model

# Reload saved inference model
inference_model = load_model("/content/updated_maize_disease_efficientnetb0.h5")
print("âœ… Inference model loaded successfully!")

from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import os
import tensorflow as tf

def predict_image_inference(img_path, model, class_names, img_size=(224,224)):
    # Load & preprocess image
    img = image.load_img(img_path, target_size=img_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)

    # Predict
    preds = model.predict(img_array, verbose=0)
    predicted_class = class_names[np.argmax(preds)]
    confidence = np.max(preds) * 100

    # Print result
    print(f"ðŸ–¼ï¸ Image: {os.path.basename(img_path)}")
    print(f"âœ… Predicted: {predicted_class} ({confidence:.2f}%)")

    # Show image with prediction
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"{predicted_class} ({confidence:.2f}%)")
    plt.show()


# Example test images
predict_image_inference("/content/Maize_Dataset/balanced_data/Gray_Leaf_Spot/Corn_Gray_Spot (104).JPG", inference_model, class_names)
predict_image_inference("/content/Maize_Dataset/balanced_data/Healthy/Corn_Health (1004).jpg", inference_model, class_names)
predict_image_inference("/content/Maize_Dataset/balanced_data/Blight/Blight_0_5995.jpg", inference_model, class_names)
predict_image_inference("/content/Maize_Dataset/balanced_data/Common_Rust/Corn_Common_Rust (1004).JPG", inference_model, class_names)
predict_image_inference("/content/Maize_Dataset/balanced_data/Gray_Leaf_Spot/Corn_Gray_Spot (124).JPG", inference_model, class_names)



